<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Papers on 泥土巢</title>
    <link>http://nituchao.com/tags/papers/index.xml</link>
    <description>Recent content in Papers on 泥土巢</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2017, nituchao | All rights reserved.</copyright>
    <atom:link href="http://nituchao.com/tags/papers/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The JSR-133 Cookbook</title>
      <link>http://nituchao.com/post/papers/jsr-133-cookbook-for-compiler-writers/</link>
      <pubDate>Tue, 11 Oct 2016 20:39:27 +0800</pubDate>
      
      <guid>http://nituchao.com/post/papers/jsr-133-cookbook-for-compiler-writers/</guid>
      <description>

&lt;h1 id=&#34;the-jsr-133-cookbook-for-compiler-writers&#34;&gt;The JSR-133 Cookbook for Compiler Writers&lt;/h1&gt;

&lt;p&gt;by Doug Lea, with help from members of the JMM mailing list.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;dl@cs.oswego.edu.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://gee.cs.oswego.edu/dl/jmm/cookbook.html&#34;&gt;origin html link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Preface: Over the 10+ years since this was initially written, many processor and language memory model specifications and issues have become clearer and better understood. And many have not. While this guide is maintained to remain accurate, it is incomplete about some of these evolving details. For more extensive coverage, see especially the work of Peter Sewell and the Cambridge Relaxed Memory Concurrency Group&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is an unofficial guide to implementing the new &lt;a href=&#34;http://www.cs.umd.edu/~pugh/java/memoryModel/&#34;&gt;Java Memory Model (JMM)&lt;/a&gt; specified by &lt;a href=&#34;http://jcp.org/en/jsr/detail?id=133&#34;&gt;JSR-133 &lt;/a&gt;. It provides at most brief backgrounds about why various rules exist, instead concentrating on their consequences for compilers and JVMs with respect to instruction reorderings, multiprocessor barrier instructions, and atomic operations. It includes a set of recommended recipes for complying to JSR-133. This guide is &amp;ldquo;unofficial&amp;rdquo; because it includes interpretations of particular processor properties and specifications. We cannot guarantee that the intepretations are correct. Also, processor specifications and implementations may change over time.&lt;/p&gt;

&lt;h2 id=&#34;reorderings&#34;&gt;Reorderings&lt;/h2&gt;

&lt;p&gt;For a compiler writer, the JMM mainly consists of rules disallowing reorderings of certain instructions that access fields (where &amp;ldquo;fields&amp;rdquo; include array elements) as well as monitors (locks).&lt;/p&gt;

&lt;h3 id=&#34;volatiles-and-monitors&#34;&gt;Volatiles and Monitors&lt;/h3&gt;

&lt;p&gt;The main JMM rules for volatiles and monitors can be viewed as a matrix with cells indicating that you cannot reorder instructions associated with particular sequences of bytecodes. This table is not itself the JMM specification; it is just a useful way of viewing its main consequences for compilers and runtime systems.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Can Reorder&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;2nd operation&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;1st operation&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Normal LoadNormal Store&lt;/td&gt;
&lt;td&gt;Volatile Load MonitorEnter&lt;/td&gt;
&lt;td&gt;Volatile Store MonitorExit&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Normal LoadNormal Store&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Volatile Load MonitorEnter&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Volatile store MonitorExit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Normal Loads are getfield, getstatic, array load of non-volatile fields.&lt;/li&gt;
&lt;li&gt;Normal Stores are putfield, putstatic, array store of non-volatile fields&lt;/li&gt;
&lt;li&gt;Volatile Loads are getfield, getstatic of volatile fields that are accessible by multiple threads&lt;/li&gt;
&lt;li&gt;Volatile Stores are putfield, putstatic of volatile fields that are accessible by multiple threads&lt;/li&gt;
&lt;li&gt;MonitorEnters (including entry to synchronized methods) are for lock objects accessible by multiple threads.&lt;/li&gt;
&lt;li&gt;MonitorExits (including exit from synchronized methods) are for lock objects accessible by multiple threads.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The cells for Normal Loads are the same as for Normal Stores, those for Volatile Loads are the same as MonitorEnter, and those for Volatile Stores are same as MonitorExit, so they are collapsed together here (but are expanded out as needed in subsequent tables). We consider here only variables that are readable and writable as an atomic unit &amp;ndash; that is, no bit fields, unaligned accesses, or accesses larger than word sizes available on a platform.&lt;/p&gt;

&lt;p&gt;Any number of other operations might be present between the indicated 1st and 2nd operations in the table. So, for example, the &amp;ldquo;No&amp;rdquo; in cell [Normal Store, Volatile Store] says that a non-volatile store cannot be reordered with ANY subsequent volatile store; at least any that can make a difference in multithreaded program semantics.&lt;/p&gt;

&lt;p&gt;The JSR-133 specification is worded such that the rules for both volatiles and monitors apply only to those that may be accessed by multiple threads. If a compiler can somehow (usually only with great effort) prove that a lock is only accessible from a single thread, it may be eliminated. Similarly, a volatile field provably accessible from only a single thread acts as a normal field. More fine-grained analyses and optimizations are also possible, for example, those relying on provable inaccessibility from multiple threads only during certain intervals.&lt;/p&gt;

&lt;p&gt;Blank cells in the table mean that the reordering is allowed if the accesses aren&amp;rsquo;t otherwise dependent with respect to basic Java semantics (as specified in the &lt;a href=&#34;http://www.javasoft.com/doc/language_specification/index.html&#34;&gt;JLS&lt;/a&gt;). For example even though the table doesn&amp;rsquo;t say so, you can&amp;rsquo;t reorder a load with a subsequent store to the same location. But you can reorder a load and store to two distinct locations, and may wish to do so in the course of various compiler transformations and optimizations. This includes cases that aren&amp;rsquo;t usually thought of as reorderings; for example reusing a computed value based on a loaded field rather than reloading and recomputing the value acts as a reordering. However, the JMM spec permits transformations that eliminate avoidable dependencies, and in turn allow reorderings.&lt;/p&gt;

&lt;p&gt;In all cases, permitted reorderings must maintain minimal Java safety properties even when accesses are incorrectly synchronized by programmers: All observed field values must be either the default zero/null &amp;ldquo;pre-construction&amp;rdquo; values, or those written by some thread. This usually entails zeroing all heap memory holding objects before it is used in constructors and never reordering other loads with the zeroing stores. A good way to do this is to zero out reclaimed memory within the garbage collector. See the JSR-133 spec for rules dealing with other corner cases surrounding safety guarantees.&lt;/p&gt;

&lt;p&gt;The rules and properties described here are for accesses to Java-level fields. In practice, these will additionally interact with accesses to internal bookkeeping fields and data, for example object headers, GC tables, and dynamically generated code.&lt;/p&gt;

&lt;h3 id=&#34;final-fields&#34;&gt;Final Fields&lt;/h3&gt;

&lt;p&gt;Loads and Stores of final fields act as &amp;ldquo;normal&amp;rdquo; accesses with respect to locks and volatiles, but impose two additional reordering rules:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A store of a final field (inside a constructor) and, if the field is a reference, any store that this final can reference, cannot be reordered with a subsequent store (outside that constructor) of the reference to the object holding that field into a variable accessible to other threads. For example, you cannot reorder
​      &lt;code&gt;x.finalField = v; ... ; sharedRef = x;&lt;/code&gt;
This comes into play for example when inlining constructors, where &amp;ldquo;&lt;code&gt;...&lt;/code&gt;&amp;rdquo; spans the logical end of the constructor. You cannot move stores of finals within constructors down below a store outside of the constructor that might make the object visible to other threads. (As seen below, this may also require issuing a barrier). Similarly, you cannot reorder either of the first two with the third assignment in:
​      &lt;code&gt;v.afield = 1; x.finalField = v; ... ; sharedRef = x;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The initial load (i.e., the very first encounter by a thread) of a final field cannot be reordered with the initial load of the reference to the object containing the final field. This comes into play in:
​      &lt;code&gt;x = sharedRef; ... ; i = x.finalField;&lt;/code&gt;
A compiler would never reorder these since they are dependent, but there can be consequences of this rule on some processors.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These rules imply that reliable use of final fields by Java programmers requires that the load of a shared reference to an object with a final field itself be synchronized, volatile, or final, or derived from such a load, thus ultimately ordering the initializing stores in constructors with subsequent uses outside constructors.&lt;/p&gt;

&lt;h2 id=&#34;memory-barriers&#34;&gt;Memory Barriers&lt;/h2&gt;

&lt;p&gt;Compilers and processors must both obey reordering rules. No particular effort is required to ensure that uniprocessors maintain proper ordering, since they all guarantee &amp;ldquo;as-if-sequential&amp;rdquo; consistency. But on multiprocessors, guaranteeing conformance often requires emitting barrier instructions. Even if a compiler optimizes away a field access (for example because a loaded value is not used), barriers must still be generated as if the access were still present. (Although see below about independently optimizing away barriers.)&lt;/p&gt;

&lt;p&gt;Memory barriers are only indirectly related to higher-level notions described in memory models such as &amp;ldquo;acquire&amp;rdquo; and &amp;ldquo;release&amp;rdquo;. And memory barriers are not themselves &amp;ldquo;synchronization barriers&amp;rdquo;. And memory barriers are unrelated to the kinds of &amp;ldquo;write barriers&amp;rdquo; used in some garbage collectors. Memory barrier instructions directly control only the interaction of a CPU with its cache, with its write-buffer that holds stores waiting to be flushed to memory, and/or its buffer of waiting loads or speculatively executed instructions. These effects may lead to further interaction among caches, main memory and other processors. But there is nothing in the JMM that mandates any particular form of communication across processors so long as stores eventually become globally performed; i.e., visible across all processors, and that loads retrieve them when they are visible.&lt;/p&gt;

&lt;h3 id=&#34;categories&#34;&gt;Categories&lt;/h3&gt;

&lt;p&gt;Nearly all processors support at least a coarse-grained barrier instruction, often just called a Fence, that guarantees that all loads and stores initiated before the fence will be strictly ordered before any load or store initiated after the fence. This is usually among the most time-consuming instructions on any given processor (often nearly as, or even more expensive than atomic instructions). Most processors additionally support more fine-grained barriers.&lt;/p&gt;

&lt;p&gt;A property of memory barriers that takes some getting used to is that they apply &lt;em&gt;BETWEEN&lt;/em&gt; memory accesses. Despite the names given for barrier instructions on some processors, the right/best barrier to use depends on the kinds of accesses it separates. Here&amp;rsquo;s a common categorization of barrier types that maps pretty well to specific instructions (sometimes no-ops) on existing processors:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;LoadLoad Barriers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The sequence: &lt;code&gt;Load1; LoadLoad; Load2&lt;/code&gt;ensures that Load1&amp;rsquo;s data are loaded before data accessed by Load2 and all subsequent load instructions are loaded. In general, explicit LoadLoad barriers are needed on processors that perform speculative loads and/or out-of-order processing in which waiting load instructions can bypass waiting stores. On processors that guarantee to always preserve load ordering, the barriers amount to no-ops.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;StoreStore Barriers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The sequence: &lt;code&gt;Store1; StoreStore; Store2&lt;/code&gt;ensures that Store1&amp;rsquo;s data are visible to other processors (i.e., flushed to memory) before the data associated with Store2 and all subsequent store instructions. In general, StoreStore barriers are needed on processors that do not otherwise guarantee strict ordering of flushes from write buffers and/or caches to other processors or main memory.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;LoadStore Barriers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The sequence: &lt;code&gt;Load1; LoadStore; Store2&lt;/code&gt;ensures that Load1&amp;rsquo;s data are loaded before all data associated with Store2 and subsequent store instructions are flushed. LoadStore barriers are needed only on those out-of-order procesors in which waiting store instructions can bypass loads.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;StoreLoad Barriers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The sequence: &lt;code&gt;Store1; StoreLoad; Load2&lt;/code&gt;ensures that Store1&amp;rsquo;s data are made visible to other processors (i.e., flushed to main memory) before data accessed by Load2 and all subsequent load instructions are loaded. StoreLoad barriers protect against a subsequent load incorrectly using Store1&amp;rsquo;s data value rather than that from a more recent store to the same location performed by a different processor. Because of this, on the processors discussed below, a StoreLoad is strictly necessary only for separating stores from subsequent loads of the &lt;em&gt;same&lt;/em&gt; location(s) as were stored before the barrier. StoreLoad barriers are needed on nearly all recent multiprocessors, and are usually the most expensive kind. Part of the reason they are expensive is that they must disable mechanisms that ordinarily bypass cache to satisfy loads from write-buffers. This might be implemented by letting the buffer fully flush, among other possible stalls.&lt;/p&gt;

&lt;p&gt;On all processors discussed below, it turns out that instructions that perform StoreLoad also obtain the other three barrier effects, so StoreLoad can serve as a general-purpose (but usually expensive) Fence. (This is an empirical fact, not a necessity.) The opposite doesn&amp;rsquo;t hold though. It is &lt;em&gt;NOT&lt;/em&gt; usually the case that issuing any combination of other barriers gives the equivalent of a StoreLoad.&lt;/p&gt;

&lt;p&gt;The following table shows how these barriers correspond to JSR-133 ordering rules.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Required barriers&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;2nd operation&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;1st operation&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Normal Load&lt;/td&gt;
&lt;td&gt;Normal Store&lt;/td&gt;
&lt;td&gt;Volatile Load MonitorEnter&lt;/td&gt;
&lt;td&gt;Volatile Store MonitorExit&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Normal Load&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;LoadStore&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Normal Store&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;StoreStore&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Volatile Load MonitorEnter&lt;/td&gt;
&lt;td&gt;LoadLoad&lt;/td&gt;
&lt;td&gt;LoadStore&lt;/td&gt;
&lt;td&gt;LoadLoad&lt;/td&gt;
&lt;td&gt;LoadStore&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Volatile Store MonitorExit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;StoreLoad&lt;/td&gt;
&lt;td&gt;StoreStore&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Plus the special final-field rule requiring a StoreStore barrier in
​      &lt;code&gt;x.finalField = v; StoreStore; sharedRef = x;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example showing placements.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Java&lt;/th&gt;
&lt;th&gt;Instructions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;class X {  int a, b;  volatile int v, u;  void f() {    int i, j;       i = a;    j = b;    i = v;       j = u;       a = i;    b = j;       v = i;       u = j;       i = u;          j = b;    a = i;  }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;load aload bload v   LoadLoadload u   LoadStorestore astore b   StoreStorestore v   StoreStorestore u   StoreLoadload u   LoadLoad   LoadStoreload bstore a&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;data-dependency-and-barriers&#34;&gt;Data Dependency and Barriers&lt;/h3&gt;

&lt;p&gt;The need for LoadLoad and LoadStore barriers on some processors interacts with their ordering guarantees for dependent instructions. On some (most) processors, a load or store that is dependent on the value of a previous load are ordered by the processor without need for an explicit barrier. This commonly arises in two kinds of cases, indirection:
​      &lt;code&gt;Load x; Load x.field&lt;/code&gt;
and control
​      &lt;code&gt;Load x; if (predicate(x)) Load or Store y;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Processors that do &lt;em&gt;NOT&lt;/em&gt; respect indirection ordering in particular require barriers for final field access for references initially obtained through shared references:
​      &lt;code&gt;x = sharedRef; ... ; LoadLoad; i = x.finalField;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Conversely, as discussed below, processors that &lt;em&gt;DO&lt;/em&gt; respect data dependencies provide several opportunities to optimize away LoadLoad and LoadStore barrier instructions that would otherwise need to be issued. (However, dependency does *NOT*automatically remove the need for StoreLoad barriers on any processor.)&lt;/p&gt;

&lt;h3 id=&#34;interactions-with-atomic-instructions&#34;&gt;Interactions with Atomic Instructions&lt;/h3&gt;

&lt;p&gt;The kinds of barriers needed on different processors further interact with implementation of MonitorEnter and MonitorExit. Locking and/or unlocking usually entail the use of atomic conditional update operations CompareAndSwap (CAS) or LoadLinked/StoreConditional (LL/SC) that have the semantics of performing a volatile load followed by a volatile store. While CAS or LL/SC minimally suffice, some processors also support other atomic instructions (for example, an unconditional exchange) that can sometimes be used instead of or in conjunction with atomic conditional updates.&lt;/p&gt;

&lt;p&gt;On all processors, atomic operations protect against read-after-write problems for the locations being read/updated. (Otherwise standard loop-until-success constructions wouldn&amp;rsquo;t work in the desired way.) But processors differ in whether atomic instructions provide more general barrier properties than the implicit StoreLoad for their target locations. On some processors these instructions also intrinsically perform barriers that would otherwise be needed for MonitorEnter/Exit; on others some or all of these barriers must be specifically issued.&lt;/p&gt;

&lt;p&gt;Volatiles and Monitors have to be separated to disentangle these effects, giving:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Required Barriers&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;2nd operation&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;1st operation&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Normal Load&lt;/td&gt;
&lt;td&gt;Normal Store&lt;/td&gt;
&lt;td&gt;Volatile Load&lt;/td&gt;
&lt;td&gt;Volatile Store&lt;/td&gt;
&lt;td&gt;MonitorEnter&lt;/td&gt;
&lt;td&gt;MonitorExit&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Normal Load&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;LoadStore&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;LoadStore&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Normal Store&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;StoreStore&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;StoreExit&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Volatile Load&lt;/td&gt;
&lt;td&gt;LoadLoad&lt;/td&gt;
&lt;td&gt;LoadStore&lt;/td&gt;
&lt;td&gt;LoadLoad&lt;/td&gt;
&lt;td&gt;LoadStore&lt;/td&gt;
&lt;td&gt;LoadEnter&lt;/td&gt;
&lt;td&gt;LoadExit&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Volatile Store&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;StoreLoad&lt;/td&gt;
&lt;td&gt;StoreStore&lt;/td&gt;
&lt;td&gt;StoreEnter&lt;/td&gt;
&lt;td&gt;StoreExit&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MonitorEnter&lt;/td&gt;
&lt;td&gt;EnterLoad&lt;/td&gt;
&lt;td&gt;EnterStore&lt;/td&gt;
&lt;td&gt;EnterLoad&lt;/td&gt;
&lt;td&gt;EnterStore&lt;/td&gt;
&lt;td&gt;EnterEnter&lt;/td&gt;
&lt;td&gt;EnterExit&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;MonitorExit&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;ExitLoad&lt;/td&gt;
&lt;td&gt;ExitStore&lt;/td&gt;
&lt;td&gt;ExitEnter&lt;/td&gt;
&lt;td&gt;ExitExit&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Plus the special final-field rule requiring a StoreStore barrier in:
​      &lt;code&gt;x.finalField = v; StoreStore; sharedRef = x;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In this table, &amp;ldquo;Enter&amp;rdquo; is the same as &amp;ldquo;Load&amp;rdquo; and &amp;ldquo;Exit&amp;rdquo; is the same as &amp;ldquo;Store&amp;rdquo;, unless overridden by the use and nature of atomic instructions. In particular:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;EnterLoad is needed on entry to any synchronized block/method that performs a load. It is the same as LoadLoad unless an atomic instruction is used in MonitorEnter and itself provides a barrier with at least the properties of LoadLoad, in which case it is a no-op.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;StoreExit is needed on exit of any synchronized block/method that performs a store. It is the same as StoreStore unless an atomic instruction is used in MonitorExit and itself provides a barrier with at least the properties of StoreStore, in which case it is a no-op.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ExitEnter&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;is the same as&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;StoreLoad&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;unless atomic instructions are used in MonitorExit and/or MonitorEnter and at least one of these provide a barrier with at least the properties of&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;StoreLoad&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;, in which case it is a no-op.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The other types are specializations that are unlikely to play a role in compilation (see below) and/or reduce to no-ops on current processors. For example, EnterEnter is needed to separate nested MonitorEnters when there are no intervening loads or stores. Here&amp;rsquo;s an example showing placements of most types:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Java&lt;/th&gt;
&lt;th&gt;Instructions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;class X {  int a;  volatile int v;  void f() {    int i;    synchronized(this) {      i = a;      a = i;    }    synchronized(this) {      synchronized(this) {      }    }    i = v;    synchronized(this) {    }    v = i;    synchronized(this) {    }  }}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;enter   EnterLoad   EnterStoreload astore a   LoadExit   StoreExitexit   ExitEnterenter   EnterEnterenter   EnterExitexit   ExitExitexit   ExitEnter   ExitLoadload v   LoadEnterenter   EnterExitexit   ExitEnter   ExitStorestore v   StoreEnterenter   EnterExitexit&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Java-level access to atomic conditional update operations will be available in JDK1.5 via &lt;a href=&#34;http://gee.cs.oswego.edu/dl/concurrency-interest/&#34;&gt;JSR-166 (concurrency utilities)&lt;/a&gt; so compilers will need to issue associated code, using a variant of the above table that collapses MonitorEnter and MonitorExit &amp;ndash; semantically, and sometimes in practice, these Java-level atomic updates act as if they are surrounded by locks.&lt;/p&gt;

&lt;h2 id=&#34;multiprocessors&#34;&gt;Multiprocessors&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s a listing of processors that are commonly used in MPs, along with links to documents providing information about them. (Some require some clicking around from the linked site and/or free registration to access manuals). This isn&amp;rsquo;t an exhaustive list, but it includes processors used in all current and near-future multiprocessor Java implementations I know of. The list and the properties of processors decribed below are not definitive. In some cases I&amp;rsquo;m just reporting what I read, and could have misread. Several reference manuals are not very clear about some properties relevant to the JMM. Please help make it definitive.&lt;/p&gt;

&lt;p&gt;Good sources of hardware-specific information about barriers and related properties of machines not listed here are &lt;a href=&#34;http://www.hpl.hp.com/research/linux/atomic_ops/&#34;&gt;Hans Boehm&amp;rsquo;s atomic_ops library&lt;/a&gt;, the &lt;a href=&#34;http://kernel.org/&#34;&gt;Linux Kernel Source&lt;/a&gt;, and &lt;a href=&#34;http://lse.sourceforge.net/&#34;&gt;Linux Scalability Effort&lt;/a&gt;. Barriers needed in the linux kernel correspond in straightforward ways to those discussed here, and have been ported to most processors. For descriptions of the underlying models supported on different processors, see &lt;a href=&#34;http://rsim.cs.uiuc.edu/~sadve/&#34;&gt;Sarita Adve et al, Recent Advances in Memory Consistency Models for Hardware Shared-Memory Systems&lt;/a&gt; and &lt;a href=&#34;http://rsim.cs.uiuc.edu/~sadve/&#34;&gt;Sarita Adve and Kourosh Gharachorloo, Shared Memory Consistency Models: A Tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sparc-TSO&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ultrasparc 1, 2, 3 (sparcv9) in TSO (Total Store Order) mode. Ultra3s only support TSO mode. (RMO mode in Ultra1/2 is never used so can be ignored.) See &lt;a href=&#34;http://www.sun.com/processors/manuals/index.html&#34;&gt;UltraSPARC III Cu User&amp;rsquo;s Manual&lt;/a&gt; and &lt;a href=&#34;http://www.sparc.com/resource.htm&#34;&gt;The SPARC Architecture Manual, Version 9 &lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;x86 (and x64)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Intel 486+, as well as AMD and apparently others. There was a flurry of re-specs in 2005-2009, but the current specs are nearly identical to TSO, differing mainly only in supporting different cache modes, and dealing with corner cases such as unaligned accesses and special forms of instructions. See &lt;a href=&#34;http://www.intel.com/products/processor/manuals/&#34;&gt;The IA-32 Intel Architecture Software Developers Manuals: System Programming Guide&lt;/a&gt; and &lt;a href=&#34;http://www.amd.com/us-en/Processors/DevelopWithAMD/0,,30_2252_875_7044,00.html&#34;&gt;AMD Architecture Programmer&amp;rsquo;s Manual Programming&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ia64&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Itanium. See &lt;a href=&#34;http://developer.intel.com/design/itanium/manuals/iiasdmanual.htm&#34;&gt;Intel Itanium Architecture Software Developer&amp;rsquo;s Manual, Volume 2: System Architecture&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ppc (POWER)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All versions have the same basic memory model, but the names and definition of some memory barrier instructions changed over time. The listed versions have been current since Power4; see architecture manuals for details. See &lt;a href=&#34;http://www.motorola.com/PowerPC/&#34;&gt;MPC603e RISC Microprocessor Users Manual&lt;/a&gt;, &lt;a href=&#34;http://www.motorola.com/PowerPC/&#34;&gt;MPC7410/MPC7400 RISC Microprocessor Users Manual &lt;/a&gt;, &lt;a href=&#34;http://www-106.ibm.com/developerworks/eserver/articles/archguide.html&#34;&gt;Book II of PowerPC Architecture Book&lt;/a&gt;, &lt;a href=&#34;http://www-3.ibm.com/chips/techlib/techlib.nsf/techdocs/F6153E213FDD912E87256D49006C6541&#34;&gt;PowerPC Microprocessor Family: Software reference manual&lt;/a&gt;, &lt;a href=&#34;http://www-3.ibm.com/chips/techlib/techlib.nsf/techdocs/852569B20050FF778525699600682CC7&#34;&gt;Book E- Enhanced PowerPC Architecture&lt;/a&gt;, &lt;a href=&#34;http://e-www.motorola.com/webapp/sps/site/overview.jsp?nodeId=03M943030450467M0ys3k3KQ&#34;&gt;EREF: A Reference for Motorola Book E and the e500 Core&lt;/a&gt;. For discussion of barriers see &lt;a href=&#34;http://www-1.ibm.com/servers/esdd/articles/power4_mem.html&#34;&gt;IBM article on power4 barriers&lt;/a&gt;, and &lt;a href=&#34;http://www-106.ibm.com/developerworks/eserver/articles/powerpc.html&#34;&gt;IBM article on powerpc barriers&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;arm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Version 7+. See &lt;a href=&#34;http://infocenter.arm.com/help/index.jsp&#34;&gt;ARM processor specifications&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;alpha&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;21264x and I think all others. See &lt;a href=&#34;http://www.alphalinux.org/docs/alphaahb.html&#34;&gt;Alpha Architecture Handbook&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;pa-risc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HP pa-risc implementations. See the &lt;a href=&#34;http://h21007.www2.hp.com/dspp/tech/tech_TechDocumentDetailPage_IDX/1,1701,2533,00.html&#34;&gt;pa-risc 2.0 Architecture&lt;/a&gt; manual.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how these processors support barriers and atomics:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Processor&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;LoadStore&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;LoadLoad&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;StoreStore&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;StoreLoad&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Datadependencyorders loads?&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;AtomicConditional&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;OtherAtomics&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Atomicsprovidebarrier?&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;sparc-TSO&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;membar(StoreLoad)&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;CAS:casa&lt;/td&gt;
&lt;td&gt;swap,ldstub&lt;/td&gt;
&lt;td&gt;full&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;x86&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;mfence or cpuid orlocked insn&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;CAS:cmpxchg&lt;/td&gt;
&lt;td&gt;xchg,locked insn&lt;/td&gt;
&lt;td&gt;full&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;ia64&lt;/td&gt;
&lt;td&gt;*combinewith*st.rel or ld.acq&lt;/td&gt;
&lt;td&gt;ld.acq&lt;/td&gt;
&lt;td&gt;st.rel&lt;/td&gt;
&lt;td&gt;mf&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;CAS:cmpxchg&lt;/td&gt;
&lt;td&gt;xchg,fetchadd&lt;/td&gt;
&lt;td&gt;target +acq/rel&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;arm&lt;/td&gt;
&lt;td&gt;dmb(see below)&lt;/td&gt;
&lt;td&gt;dmb(see below)&lt;/td&gt;
&lt;td&gt;dmb-st&lt;/td&gt;
&lt;td&gt;dmb&lt;/td&gt;
&lt;td&gt;indirectiononly&lt;/td&gt;
&lt;td&gt;LL/SC:ldrex/strex&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;targetonly&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;ppc&lt;/td&gt;
&lt;td&gt;lwsync(see below)&lt;/td&gt;
&lt;td&gt;hwsync(see below)&lt;/td&gt;
&lt;td&gt;lwsync&lt;/td&gt;
&lt;td&gt;hwsync&lt;/td&gt;
&lt;td&gt;indirectiononly&lt;/td&gt;
&lt;td&gt;LL/SC:ldarx/stwcx&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;targetonly&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;alpha&lt;/td&gt;
&lt;td&gt;mb&lt;/td&gt;
&lt;td&gt;mb&lt;/td&gt;
&lt;td&gt;wmb&lt;/td&gt;
&lt;td&gt;mb&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;LL/SC:ldx_l/stx_c&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;targetonly&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;pa-risc&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;no-op&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;*buildfrom*ldcw&lt;/td&gt;
&lt;td&gt;ldcw&lt;/td&gt;
&lt;td&gt;&lt;em&gt;(NA)&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;notes&#34;&gt;Notes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Some of the listed barrier instructions have stronger properties than actually needed in the indicated cells, but seem to be the cheapest way to get desired effects.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The listed barrier instructions are those designed for use with normal program memory, but not necessarily other special forms/modes of caching and memory used for IO and system tasks. For example, on x86-SPO, StoreStore barriers (&amp;ldquo;sfence&amp;rdquo;) are needed with WriteCombining (WC) caching mode, which is designed for use in system-level bulk transfers etc. OSes use Writeback mode for programs and data, which doesn&amp;rsquo;t require StoreStore barriers.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On x86, any lock-prefixed instruction can be used as a StoreLoad barrier. (The form used in linux kernels is the no-op &lt;code&gt;lock; addl $0,0(%%esp)&lt;/code&gt;.) Versions supporting the &amp;ldquo;SSE2&amp;rdquo; extensions (Pentium4 and later) support the mfence instruction which seems preferable unless a lock-prefixed instruction like CAS is needed anyway. The cpuid instruction also works but is slower.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On ia64, LoadStore, LoadLoad and StoreStore barriers are folded into special forms of load and store instructions &amp;ndash; there aren&amp;rsquo;t separate instructions. ld.acq acts as (load; LoadLoad+LoadStore) and st.rel acts as (LoadStore+StoreStore; store). Neither of these provide a StoreLoad barrier &amp;ndash; you need a separate mf barrier instruction for that.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On both ARM and ppc, there may be opportunities to replace load fences in the presence of data dependencies with non-fence-based instruction sequences. Sequences and cases in which they apply are described in work by the &lt;a href=&#34;http://www.cl.cam.ac.uk/~pes20/ppc-supplemental/&#34;&gt;Cambridge Relaxed Memory Concurrency Group&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The sparc membar instruction supports all four barrier modes, as well as combinations of modes. But only the StoreLoad mode is ever needed in TSO. On some UltraSparcs, &lt;em&gt;any&lt;/em&gt; membar instruction produces the effects of a StoreLoad, regardless of mode.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The x86 processors supporting &amp;ldquo;streaming SIMD&amp;rdquo; SSE2 extensions require LoadLoad &amp;ldquo;lfence&amp;rdquo; &lt;em&gt;only&lt;/em&gt; only in connection with these streaming instructions.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Although the pa-risc specification does not mandate it, all HP pa-risc implementations are sequentially consistent, so have no memory barrier instructions.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The only atomic primitive on pa-risc is ldcw, a form of test-and-set, from which you would need to build up atomic conditional updates using techniques such as those in the &lt;a href=&#34;http://h21007.www2.hp.com/hpux-devtools/CXX/hpux-devtools.0106/0014.html&#34;&gt;HP white paper on spinlocks&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CAS and LL/SC take multiple forms on different processors, differing only with respect to field width, minimially including 4 and 8 byte versions.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On sparc and x86, CAS has implicit preceding and trailing full StoreLoad barriers. The sparcv9 architecture manual says CAS need not have post-StoreLoad barrier property, but the chip manuals indicate that it does on ultrasparcs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On ppc and alpha, LL/SC have implicit barriers only with respect to the locations being loaded/stored, but don&amp;rsquo;t have more general barrier properties.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The ia64 cmpxchg instruction also has implicit barriers with respect to the locations being loaded/stored, but additionally takes an optional .acq (post-LoadLoad+LoadStore) or .rel (pre-StoreStore+LoadStore) modifier. The form cmpxchg.acq can be used for MonitorEnter, and cmpxchg.rel for MonitorExit. In those cases where exits and enters are not guaranteed to be matched, an ExitEnter (StoreLoad) barrier may also be needed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sparc, x86 and ia64 support unconditional-exchange (swap, xchg). Sparc ldstub is a one-byte test-and-set. ia64 fetchadd returns previous value and adds to it. On x86, several instructions (for example add-to-memory) can be lock-prefixed, causing them to act atomically.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;recipes&#34;&gt;Recipes&lt;/h2&gt;

&lt;h3 id=&#34;uniprocessors&#34;&gt;Uniprocessors&lt;/h3&gt;

&lt;p&gt;If you are generating code that is guaranteed to only run on a uniprocessor, then you can probably skip the rest of this section. Because uniprocessors preserve apparent sequential consistency, you never need to issue barriers unless object memory is somehow shared with asynchrononously accessible IO memory. This might occur with specially mapped java.nio buffers, but probably only in ways that affect internal JVM support code, not Java code. Also, it is conceivable that some special barriers would be needed if context switching doesn&amp;rsquo;t entail sufficient synchronization.&lt;/p&gt;

&lt;h3 id=&#34;inserting-barriers&#34;&gt;Inserting Barriers&lt;/h3&gt;

&lt;p&gt;Barrier instructions apply&lt;/p&gt;

&lt;p&gt;between&lt;/p&gt;

&lt;p&gt;different kinds of accesses as they occur during execution of a program. Finding an &amp;ldquo;optimal&amp;rdquo; placement that minimizes the total number of executed barriers is all but impossible. Compilers often cannot tell if a given load or store will be preceded or followed by another that requires a barrier; for example, when a volatile store is followed by a return. The easiest conservative strategy is to assume that the kind of access requiring the &amp;ldquo;heaviest&amp;rdquo; kind of barrier will occur when generating code for any given load, store, lock, or unlock:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Issue a StoreStore barrier before each volatile store.
(On ia64 you must instead fold this and most barriers into corresponding load or store instructions.)&lt;/li&gt;
&lt;li&gt;Issue a StoreStore barrier after all stores but before return from any constructor for any class with a final field.&lt;/li&gt;
&lt;li&gt;Issue a StoreLoad barrier after each volatile store.
Note that you could instead issue one before each volatile load, but this would be slower for typical programs using volatiles in which reads greatly outnumber writes. Alternatively, if available, you can implement volatile store as an atomic instruction (for example XCHG on x86) and omit the barrier. This may be more efficient if atomic instructions are cheaper than StoreLoad barriers.&lt;/li&gt;
&lt;li&gt;Issue LoadLoad and LoadStore barriers after each volatile load.
On processors that preserve data dependent ordering, you need not issue a barrier if the next access instruction is dependent on the value of the load. In particular, you do not need a barrier after a load of a volatile reference if the subsequent instruction is a null-check or load of a field of that reference.&lt;/li&gt;
&lt;li&gt;Issue an ExitEnter barrier either before each MonitorEnter or after each MonitorExit.
(As discussed above, ExitEnter is a no-op if either MonitorExit or MonitorEnter uses an atomic instruction that supplies the equivalent of a StoreLoad barrier. Similarly for others involving Enter and Exit in the remaining steps.)&lt;/li&gt;
&lt;li&gt;Issue EnterLoad and EnterStore barriers after each MonitorEnter.&lt;/li&gt;
&lt;li&gt;Issue StoreExit and LoadExit barriers before each MonitorExit.&lt;/li&gt;
&lt;li&gt;If on a processor that does not intrinsically provide ordering on indirect loads, issue a LoadLoad barrier before each load of a final field. (Some alternative strategies are discussed in &lt;a href=&#34;http://www.cs.umd.edu/~pugh/java/memoryModel/archive/0180.html&#34;&gt;this JMM list posting&lt;/a&gt;, and &lt;a href=&#34;http://lse.sourceforge.net/locking/wmbdd.html&#34;&gt;this description of linux data dependent barriers&lt;/a&gt;.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Many of these barriers usually reduce to no-ops. In fact, most of them reduce to no-ops, but in different ways under different processors and locking schemes. For the simplest examples, basic conformance to JSR-133 on x86 or sparc-TSO using CAS for locking amounts only to placing a StoreLoad barrier after volatile stores.&lt;/p&gt;

&lt;h3 id=&#34;removing-barriers&#34;&gt;Removing Barriers&lt;/h3&gt;

&lt;p&gt;The conservative strategy above is likely to perform acceptably for many programs. The main performance issues surrounding volatiles occur for the StoreLoad barriers associated with stores. These ought to be relatively rare &amp;ndash; the main reason for using volatiles in concurrent programs is to avoid the need to use locks around reads, which is only an issue when reads greatly overwhelm writes. But this strategy can be improved in at least the following ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Removing redundant barriers. The above tables indicate that barriers can be eliminated as follows:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;p&gt;| Original   | =&amp;gt;                  | Transformed |      |           |                     |            |
  | &amp;mdash;&amp;mdash;&amp;mdash;- | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- | &amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; | &amp;mdash;- | &amp;mdash;&amp;mdash;&amp;mdash; | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- | &amp;mdash;&amp;mdash;&amp;mdash;- |
  | 1st        | ops                 | 2nd         | =&amp;gt;   | 1st       | ops                 | 2nd        |
  | LoadLoad   | [no loads]          | LoadLoad    | =&amp;gt;   |           | [no loads]          | LoadLoad   |
  | LoadLoad   | [no loads]          | StoreLoad   | =&amp;gt;   |           | [no loads]          | StoreLoad  |
  | StoreStore | [no stores]         | StoreStore  | =&amp;gt;   |           | [no stores]         | StoreStore |
  | StoreStore | [no stores]         | StoreLoad   | =&amp;gt;   |           | [no stores]         | StoreLoad  |
  | StoreLoad  | [no loads]          | LoadLoad    | =&amp;gt;   | StoreLoad | [no loads]          |            |
  | StoreLoad  | [no stores]         | StoreStore  | =&amp;gt;   | StoreLoad | [no stores]         |            |
  | StoreLoad  | [no volatile loads] | StoreLoad   | =&amp;gt;   |           | [no volatile loads] | StoreLoad  |&lt;/p&gt;

&lt;p&gt;Similar eliminations can be used for interactions with locks, but depend on how locks are implemented. Doing all this in the presence of loops, calls, and branches is left as an exercise for the reader. :-)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Rearranging code (within the allowed constraints) to further enable removing LoadLoad and LoadStore barriers that are not needed because of data dependencies on processors that preserve such orderings.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Moving the point in the instruction stream that the barriers are issued, to improve scheduling, so long as they still occur somewhere in the interval they are required.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Removing barriers that aren&amp;rsquo;t needed because there is no possibility that multiple threads could rely on them; for example volatiles that are provably visible only from a single thread. Also, removing some barriers when it can be proven that threads can only store or only load certain fields. All this usually requires a fair amount of analysis.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;miscellany&#34;&gt;Miscellany&lt;/h3&gt;

&lt;p&gt;JSR-133 also addresses a few other issues that may entail barriers in more specialized cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Thread.start() requires barriers ensuring that the started thread sees all stores visible to the caller at the call point. Conversely, Thread.join() requires barriers ensuring that the caller sees all stores by the terminating thread. These are normally generated by the synchronization entailed in implementations of these constructs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Static final initialization requires StoreStore barriers that are normally entailed in mechanics needed to obey Java class loading and initialization rules.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ensuring default zero/null initial field values normally entails barriers, synchronization, and/or low-level cache control within garbage collectors.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;JVM-private routines that &amp;ldquo;magically&amp;rdquo; set System.in, System.out, and System.err outside of constructors or static initializers need special attention since they are special legacy exceptions to JMM rules for final fields.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Similarly, internal JVM deserialization code that sets final fields normally requires a StoreStore barrier.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Finalization support may require barriers (within garbage collectors) to ensure that Object.finalize code sees all stores to all fields prior to the objects becoming unreferenced. This is usually ensured via the synchronization used to add and remove references in reference queues.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Calls to and returns from JNI routines may require barriers, although this seems to be a quality of implementation issue.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;​&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Most processors have other synchronizing instructions designed primarily for use with IO and OS actions. These don&amp;rsquo;t impact JMM issues directly, but may be involved in IO, class loading, and dynamic code generation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;Thanks to Bill Pugh, Dave Dice, Jeremy Manson, Kourosh Gharachorloo, Tim Harris, Cliff Click, Allan Kielstra, Yue Yang, Hans Boehm, Kevin Normoyle, Juergen Kreileder, Alexander Terekhov, Tom Deneau, Clark Verbrugge, Peter Kessler, Peter Sewell, Jan Vitek, and Richard Grisenthwaite for corrections and suggestions.&lt;/p&gt;

&lt;p&gt;A translation of this page is available in &lt;a href=&#34;http://www.javareading.com/bof/cookbook-J20060917.html&#34;&gt;japanese&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;http://gee.cs.oswego.edu/dl&#34;&gt;Doug Lea&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Last modified: Tue Mar 22 07:11:36 EDT 2011&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JSR 133 (Java Memory Model) FAQ</title>
      <link>http://nituchao.com/post/papers/jsr-133-faq/</link>
      <pubDate>Mon, 10 Oct 2016 20:38:27 +0800</pubDate>
      
      <guid>http://nituchao.com/post/papers/jsr-133-faq/</guid>
      <description>

&lt;h2 id=&#34;jsr-133-java-memory-model-faq&#34;&gt;JSR 133 (Java Memory Model) FAQ&lt;/h2&gt;

&lt;p&gt;Jeremy Manson and Brian Goetz, February 2004&lt;/p&gt;

&lt;h3 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#whatismm&#34;&gt;What is a memory model, anyway?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#otherlanguages&#34;&gt;Do other languages, like C++, have a memory model?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#jsr133&#34;&gt;What is JSR 133 about?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#reordering&#34;&gt;What is meant by reordering?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#oldmm&#34;&gt;What was wrong with the old memory model?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#incorrectlySync&#34;&gt;What do you mean by incorrectly synchronized?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#synchronization&#34;&gt;What does synchronization do?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#finalWrong&#34;&gt;How can final fields appear to change their values?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#finalRight&#34;&gt;How do final fields work under the new JMM?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#volatile&#34;&gt;What does volatile do?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#dcl&#34;&gt;Does the new memory model fix the &amp;ldquo;double-checked locking&amp;rdquo; problem?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#vmwriters&#34;&gt;What if I&amp;rsquo;m writing a VM?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#conclusion&#34;&gt;Why should I care?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-is-a-memory-model-anyway&#34;&gt;What is a memory model, anyway?&lt;/h3&gt;

&lt;p&gt;In multiprocessor systems, processors generally have one or more layers of memory cache, which improves performance both by speeding access to data (because the data is closer to the processor) and reducing traffic on the shared memory bus (because many memory operations can be satisfied by local caches.) Memory caches can improve performance tremendously, but they present a host of new challenges. What, for example, happens when two processors examine the same memory location at the same time? Under what conditions will they see the same value?&lt;/p&gt;

&lt;p&gt;At the processor level, a memory model defines necessary and sufficient conditions for knowing that writes to memory by other processors are visible to the current processor, and writes by the current processor are visible to other processors. Some processors exhibit a strong memory model, where all processors see exactly the same value for any given memory location at all times. Other processors exhibit a weaker memory model, where special instructions, called memory barriers, are required to flush or invalidate the local processor cache in order to see writes made by other processors or make writes by this processor visible to others. These memory barriers are usually performed when lock and unlock actions are taken; they are invisible to programmers in a high level language.&lt;/p&gt;

&lt;p&gt;It can sometimes be easier to write programs for strong memory models, because of the reduced need for memory barriers. However, even on some of the strongest memory models, memory barriers are often necessary; quite frequently their placement is counterintuitive. Recent trends in processor design have encouraged weaker memory models, because the relaxations they make for cache consistency allow for greater scalability across multiple processors and larger amounts of memory.&lt;/p&gt;

&lt;p&gt;The issue of when a write becomes visible to another thread is compounded by the compiler&amp;rsquo;s reordering of code. For example, the compiler might decide that it is more efficient to move a write operation later in the program; as long as this code motion does not change the program&amp;rsquo;s semantics, it is free to do so.  If a compiler defers an operation, another thread will not see it until it is performed; this mirrors the effect of caching.&lt;/p&gt;

&lt;p&gt;Moreover, writes to memory can be moved earlier in a program; in this case, other threads might see a write before it actually &amp;ldquo;occurs&amp;rdquo; in the program.  All of this flexibility is by design &amp;ndash; by giving the compiler, runtime, or hardware the flexibility to execute operations in the optimal order, within the bounds of the memory model, we can achieve higher performance.&lt;/p&gt;

&lt;p&gt;A simple example of this can be seen in the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Class Reordering {
  int x = 0, y = 0;
  public void writer() {
    x = 1;
    y = 2;
  }

  public void reader() {
    int r1 = y;
    int r2 = x;
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s say that this code is executed in two threads concurrently, and the read of y sees the value 2. Because this write came after the write to x, the programmer might assume that the read of x must see the value 1. However, the writes may have been reordered. If this takes place, then the write to y could happen, the reads of both variables could follow, and then the write to x could take place. The result would be that r1 has the value 2, but r2 has the value 0.&lt;/p&gt;

&lt;p&gt;The Java Memory Model describes what behaviors are legal in multithreaded code, and how threads may interact through memory. It describes the relationship between variables in a program and the low-level details of storing and retrieving them to and from memory or registers in a real computer system. It does this in a way that can be implemented correctly using a wide variety of hardware and a wide variety of compiler optimizations.&lt;/p&gt;

&lt;p&gt;Java includes several language constructs, including volatile, final, and synchronized, which are intended to help the programmer describe a program&amp;rsquo;s concurrency requirements to the compiler. The Java Memory Model defines the behavior of volatile and synchronized, and, more importantly, ensures that a correctly synchronized Java program runs correctly on all processor architectures.&lt;/p&gt;

&lt;h3 id=&#34;do-other-languages-like-c-have-a-memory-model&#34;&gt;Do other languages, like C++, have a memory model?&lt;/h3&gt;

&lt;p&gt;Most other programming languages, such as C and C++, were not designed with direct support for multithreading. The protections that these languages offer against the kinds of reorderings that take place in compilers and architectures are heavily dependent on the guarantees provided by the threading libraries used (such as pthreads), the compiler used, and the platform on which the code is run.&lt;/p&gt;

&lt;h3 id=&#34;what-is-jsr-133-about&#34;&gt;What is JSR 133 about?&lt;/h3&gt;

&lt;p&gt;Since 1997, several serious flaws have been discovered in the Java Memory Model as defined in Chapter 17 of the Java Language Specification. These flaws allowed for confusing behaviors (such as final fields being observed to change their value) and undermined the compiler&amp;rsquo;s ability to perform common optimizations.&lt;/p&gt;

&lt;p&gt;The Java Memory Model was an ambitious undertaking; it was the first time that a programming language specification attempted to incorporate a memory model which could provide consistent semantics for concurrency across a variety of architectures. Unfortunately, defining a memory model which is both consistent and intuitive proved far more difficult than expected. JSR 133 defines a new memory model for the Java language which fixes the flaws of the earlier memory model. In order to do this, the semantics of final and volatile needed to change.&lt;/p&gt;

&lt;p&gt;The full semantics are available at &lt;a href=&#34;http://www.cs.umd.edu/users/pugh/java/memoryModel&#34;&gt;http://www.cs.umd.edu/users/pugh/java/memoryModel&lt;/a&gt;, but the formal semantics are not for the timid. It is surprising, and sobering, to discover how complicated seemingly simple concepts like synchronization really are. Fortunately, you need not understand the details of the formal semantics &amp;ndash; the goal of JSR 133 was to create a set of formal semantics that provides an intuitive framework for how volatile, synchronized, and final work.&lt;/p&gt;

&lt;p&gt;The goals of JSR 133 include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Preserving existing safety guarantees, like type-safety, and strengthening others. For example, variable values may not be created &amp;ldquo;out of thin air&amp;rdquo;: each value for a variable observed by some thread must be a value that can reasonably be placed there by some thread.&lt;/li&gt;
&lt;li&gt;The semantics of correctly synchronized programs should be as simple and intuitive as possible.&lt;/li&gt;
&lt;li&gt;The semantics of incompletely or incorrectly synchronized programs should be defined so that potential security hazards are minimized.&lt;/li&gt;
&lt;li&gt;Programmers should be able to reason confidently about how multithreaded programs interact with memory.&lt;/li&gt;
&lt;li&gt;It should be possible to design correct, high performance JVM implementations across a wide range of popular hardware architectures.&lt;/li&gt;
&lt;li&gt;A new guarantee of &lt;em&gt;initialization safety&lt;/em&gt; should be provided. If an object is properly constructed (which means that references to it do not escape during construction), then all threads which see a reference to that object will also see the values for its final fields that were set in the constructor, without the need for synchronization.&lt;/li&gt;
&lt;li&gt;There should be minimal impact on existing code.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-is-meant-by-reordering&#34;&gt;What is meant by reordering?&lt;/h3&gt;

&lt;p&gt;There are a number of cases in which accesses to program variables (object instance fields, class static fields, and array elements) may appear to execute in a different order than was specified by the program. The compiler is free to take liberties with the ordering of instructions in the name of optimization. Processors may execute instructions out of order under certain circumstances. Data may be moved between registers, processor caches, and main memory in different order than specified by the program.&lt;/p&gt;

&lt;p&gt;For example, if a thread writes to field &lt;code&gt;a&lt;/code&gt; and then to field &lt;code&gt;b&lt;/code&gt;, and the value of &lt;code&gt;b&lt;/code&gt; does not depend on the value of &lt;code&gt;a&lt;/code&gt;, then the compiler is free to reorder these operations, and the cache is free to flush &lt;code&gt;b&lt;/code&gt; to main memory before &lt;code&gt;a&lt;/code&gt;. There are a number of potential sources of reordering, such as the compiler, the JIT, and the cache.&lt;/p&gt;

&lt;p&gt;The compiler, runtime, and hardware are supposed to conspire to create the illusion of as-if-serial semantics, which means that in a single-threaded program, the program should not be able to observe the effects of reorderings. However, reorderings can come into play in incorrectly synchronized multithreaded programs, where one thread is able to observe the effects of other threads, and may be able to detect that variable accesses become visible to other threads in a different order than executed or specified in the program.&lt;/p&gt;

&lt;p&gt;Most of the time, one thread doesn&amp;rsquo;t care what the other is doing. But when it does, that&amp;rsquo;s what synchronization is for.&lt;/p&gt;

&lt;h3 id=&#34;what-was-wrong-with-the-old-memory-model&#34;&gt;What was wrong with the old memory model?&lt;/h3&gt;

&lt;p&gt;There were several serious problems with the old memory model. It was difficult to understand, and therefore widely violated. For example, the old model did not, in many cases, allow the kinds of reorderings that took place in every JVM. This confusion about the implications of the old model was what compelled the formation of JSR-133.&lt;/p&gt;

&lt;p&gt;One widely held belief, for example, was that if final fields were used, then synchronization between threads was unnecessary to guarantee another thread would see the value of the field. While this is a reasonable assumption and a sensible behavior, and indeed how we would want things to work, under the old memory model, it was simply not true. Nothing in the old memory model treated final fields differently from any other field &amp;ndash; meaning synchronization was the only way to ensure that all threads see the value of a final field that was written by the constructor. As a result, it was possible for a thread to see the default value of the field, and then at some later time see its constructed value. This means, for example, that immutable objects like String can appear to change their value &amp;ndash; a disturbing prospect indeed.&lt;/p&gt;

&lt;p&gt;The old memory model allowed for volatile writes to be reordered with nonvolatile reads and writes, which was not consistent with most developers intuitions about volatile and therefore caused confusion.&lt;/p&gt;

&lt;p&gt;Finally, as we shall see, programmers&amp;rsquo; intuitions about what can occur when their programs are incorrectly synchronized are often mistaken. One of the goals of JSR-133 is to call attention to this fact.&lt;/p&gt;

&lt;h3 id=&#34;what-do-you-mean-by-incorrectly-synchronized&#34;&gt;What do you mean by “incorrectly synchronized”?&lt;/h3&gt;

&lt;p&gt;Incorrectly synchronized code can mean different things to different people. When we talk about incorrectly synchronized code in the context of the Java Memory Model, we mean any code where&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;there is a write of a variable by one thread,&lt;/li&gt;
&lt;li&gt;there is a read of the same variable by another thread and&lt;/li&gt;
&lt;li&gt;the write and read are not ordered by synchronization&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When these rules are violated, we say we have a &lt;em&gt;data race&lt;/em&gt; on that variable. A program with a data race is an incorrectly synchronized program.&lt;/p&gt;

&lt;h3 id=&#34;what-does-synchronization-do&#34;&gt;What does synchronization do?&lt;/h3&gt;

&lt;p&gt;Synchronization has several aspects. The most well-understood is mutual exclusion &amp;ndash; only one thread can hold a monitor at once, so synchronizing on a monitor means that once one thread enters a synchronized block protected by a monitor, no other thread can enter a block protected by that monitor until the first thread exits the synchronized block.&lt;/p&gt;

&lt;p&gt;But there is more to synchronization than mutual exclusion. Synchronization ensures that memory writes by a thread before or during a synchronized block are made visible in a predictable manner to other threads which synchronize on the same monitor. After we exit a synchronized block, we **release **the monitor, which has the effect of flushing the cache to main memory, so that writes made by this thread can be visible to other threads. Before we can enter a synchronized block, we &lt;strong&gt;acquire&lt;/strong&gt; the monitor, which has the effect of invalidating the local processor cache so that variables will be reloaded from main memory. We will then be able to see all of the writes made visible by the previous release.&lt;/p&gt;

&lt;p&gt;Discussing this in terms of caches, it may sound as if these issues only affect multiprocessor machines. However, the reordering effects can be easily seen on a single processor. It is not possible, for example, for the compiler to move your code before an acquire or after a release. When we say that acquires and releases act on caches, we are using shorthand for a number of possible effects.&lt;/p&gt;

&lt;p&gt;The new memory model semantics create a partial ordering on memory operations (read field, write field, lock, unlock) and other thread operations (start and join), where some actions are said to &lt;em&gt;happen before&lt;/em&gt; other operations. When one action happens before another, the first is guaranteed to be ordered before and visible to the second. The rules of this ordering are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each action in a thread happens before every action in that thread that comes later in the program&amp;rsquo;s order.&lt;/li&gt;
&lt;li&gt;An unlock on a monitor happens before every subsequent lock on &lt;strong&gt;that same&lt;/strong&gt; monitor.&lt;/li&gt;
&lt;li&gt;A write to a volatile field happens before every subsequent read of &lt;strong&gt;that same&lt;/strong&gt; volatile.&lt;/li&gt;
&lt;li&gt;A call to &lt;code&gt;start()&lt;/code&gt; on a thread happens before any actions in the started thread.&lt;/li&gt;
&lt;li&gt;All actions in a thread happen before any other thread successfully returns from a &lt;code&gt;join()&lt;/code&gt;on that thread.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This means that any memory operations which were visible to a thread before exiting a synchronized block are visible to any thread after it enters a synchronized block protected by the same monitor, since all the memory operations happen before the release, and the release happens before the acquire.&lt;/p&gt;

&lt;p&gt;Another implication is that the following pattern, which some people use to force a memory barrier, doesn&amp;rsquo;t work:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;synchronized (new Object()) {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is actually a no-op, and your compiler can remove it entirely, because the compiler knows that no other thread will synchronize on the same monitor. You have to set up a happens-before relationship for one thread to see the results of another.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; Note that it is important for both threads to synchronize on the same monitor in order to set up the happens-before relationship properly. It is not the case that everything visible to thread A when it synchronizes on object X becomes visible to thread B after it synchronizes on object Y. The release and acquire have to &amp;ldquo;match&amp;rdquo; (i.e., be performed on the same monitor) to have the right semantics. Otherwise, the code has a data race.&lt;/p&gt;

&lt;h3 id=&#34;how-can-final-fields-appear-to-change-their-values&#34;&gt;How can final fields appear to change their values?&lt;/h3&gt;

&lt;p&gt;One of the best examples of how final fields&amp;rsquo; values can be seen to change involves one particular implementation of the &lt;code&gt;String&lt;/code&gt; class.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;String&lt;/code&gt; can be implemented as an object with three fields &amp;ndash; a character array, an offset into that array, and a length. The rationale for implementing &lt;code&gt;String&lt;/code&gt; this way, instead of having only the character array, is that it lets multiple &lt;code&gt;String&lt;/code&gt; and&lt;code&gt;StringBuffer&lt;/code&gt; objects share the same character array and avoid additional object allocation and copying. So, for example, the method &lt;code&gt;String.substring()&lt;/code&gt; can be implemented by creating a new string which shares the same character array with the original &lt;code&gt;String&lt;/code&gt; and merely differs in the length and offset fields. For a &lt;code&gt;String&lt;/code&gt;, these fields are all final fields.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;String s1 = &amp;quot;/usr/tmp&amp;quot;;
String s2 = s1.substring(4); 

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The string &lt;code&gt;s2&lt;/code&gt; will have an offset of 4 and a length of 4. But, under the old model, it was possible for another thread to see the offset as having the default value of 0, and then later see the correct value of 4, it will appear as if the string &amp;ldquo;/usr&amp;rdquo; changes to &amp;ldquo;/tmp&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The original Java Memory Model allowed this behavior; several JVMs have exhibited this behavior. The new Java Memory Model makes this illegal.&lt;/p&gt;

&lt;h3 id=&#34;how-do-final-fields-work-under-the-new-jmm&#34;&gt;How do final fields work under the new JMM?&lt;/h3&gt;

&lt;p&gt;The values for an object&amp;rsquo;s final fields are set in its constructor. Assuming the object is constructed &amp;ldquo;correctly&amp;rdquo;, once an object is constructed, the values assigned to the final fields in the constructor will be visible to all other threads without synchronization. In addition, the visible values for any other object or array referenced by those final fields will be at least as up-to-date as the final fields.&lt;/p&gt;

&lt;p&gt;What does it mean for an object to be properly constructed? It simply means that no reference to the object being constructed is allowed to &amp;ldquo;escape&amp;rdquo; during construction. (See &lt;a href=&#34;http://www-106.ibm.com/developerworks/java/library/j-jtp0618.html&#34;&gt;Safe Construction Techniques&lt;/a&gt; for examples.)  In other words, do not place a reference to the object being constructed anywhere where another thread might be able to see it; do not assign it to a static field, do not register it as a listener with any other object, and so on. These tasks should be done after the constructor completes, not in the constructor.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class FinalFieldExample {
  final int x;
  int y;
  static FinalFieldExample f;
  public FinalFieldExample() {
    x = 3;
    y = 4;
  }

  static void writer() {
    f = new FinalFieldExample();
  }

  static void reader() {
    if (f != null) {
      int i = f.x;
      int j = f.y;
    }
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The class above is an example of how final fields should be used. A thread executing &lt;code&gt;reader&lt;/code&gt; is guaranteed to see the value 3 for &lt;code&gt;f.x&lt;/code&gt;, because it is final. It is not guaranteed to see the value 4 for &lt;code&gt;y&lt;/code&gt;, because it is not final. If &lt;code&gt;FinalFieldExample&lt;/code&gt;&amp;rsquo;s constructor looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public FinalFieldExample() { // bad!
  x = 3;
  y = 4;
  // bad construction - allowing this to escape
  global.obj = this;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then threads that read the reference to &lt;code&gt;this&lt;/code&gt;from &lt;code&gt;global.obj&lt;/code&gt; are &lt;strong&gt;not&lt;/strong&gt; guaranteed to see 3 for &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The ability to see the correctly constructed value for the field is nice, but if the field itself is a reference, then you also want your code to see the up to date values for the object (or array) to which it points. If your field is a final field, this is also guaranteed. So, you can have a final pointer to an array and not have to worry about other threads seeing the correct values for the array reference, but incorrect values for the contents of the array. Again, by &amp;ldquo;correct&amp;rdquo; here, we mean &amp;ldquo;up to date as of the end of the object&amp;rsquo;s constructor&amp;rdquo;, not &amp;ldquo;the latest value available&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Now, having said all of this, if, after a thread constructs an immutable object (that is, an object that only contains final fields), you want to ensure that it is seen correctly by all of the other thread, you &lt;strong&gt;still&lt;/strong&gt; typically need to use synchronization. There is no other way to ensure, for example, that the reference to the immutable object will be seen by the second thread. The guarantees the program gets from final fields should be carefully tempered with a deep and careful understanding of how concurrency is managed in your code.&lt;/p&gt;

&lt;p&gt;There is no defined behavior if you want to use JNI to change final fields.&lt;/p&gt;

&lt;h3 id=&#34;what-does-volatile-do&#34;&gt;What does volatile do?&lt;/h3&gt;

&lt;p&gt;Volatile fields are special fields which are used for communicating state between threads. Each read of a volatile will see the last write to that volatile by any thread; in effect, they are designated by the programmer as fields for which it is never acceptable to see a &amp;ldquo;stale&amp;rdquo; value as a result of caching or reordering. The compiler and runtime are prohibited from allocating them in registers. They must also ensure that after they are written, they are flushed out of the cache to main memory, so they can immediately become visible to other threads. Similarly, before a volatile field is read, the cache must be invalidated so that the value in main memory, not the local processor cache, is the one seen. There are also additional restrictions on reordering accesses to volatile variables.&lt;/p&gt;

&lt;p&gt;Under the old memory model, accesses to volatile variables could not be reordered with each other, but they could be reordered with nonvolatile variable accesses. This undermined the usefulness of volatile fields as a means of signaling conditions from one thread to another.&lt;/p&gt;

&lt;p&gt;Under the new memory model, it is still true that volatile variables cannot be reordered with each other. The difference is that it is now no longer so easy to reorder normal field accesses around them. Writing to a volatile field has the same memory effect as a monitor release, and reading from a volatile field has the same memory effect as a monitor acquire. In effect, because the new memory model places stricter constraints on reordering of volatile field accesses with other field accesses, volatile or not, anything that was visible to thread A when it writes to volatile field &lt;code&gt;f&lt;/code&gt; becomes visible to thread B when it reads &lt;code&gt;f&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here is a simple example of how volatile fields can be used:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class VolatileExample {
  int x = 0;
  volatile boolean v = false;
  public void writer() {
    x = 42;
    v = true;
  }

  public void reader() {
    if (v == true) {
      //uses x - guaranteed to see 42.
    }
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assume that one thread is calling &lt;code&gt;writer&lt;/code&gt;, and another is calling &lt;code&gt;reader&lt;/code&gt;. The write to &lt;code&gt;v&lt;/code&gt; in &lt;code&gt;writer&lt;/code&gt; releases the write to &lt;code&gt;x&lt;/code&gt; to memory, and the read of &lt;code&gt;v&lt;/code&gt; acquires that value from memory. Thus, if the reader sees the value &lt;code&gt;true&lt;/code&gt; for v, it is also guaranteed to see the write to 42 that happened before it. This would not have been true under the old memory model.  If &lt;code&gt;v&lt;/code&gt; were not volatile, then the compiler could reorder the writes in &lt;code&gt;writer&lt;/code&gt;, and &lt;code&gt;reader&lt;/code&gt;&amp;rsquo;s read of &lt;code&gt;x&lt;/code&gt; might see 0.&lt;/p&gt;

&lt;p&gt;Effectively, the semantics of volatile have been strengthened substantially, almost to the level of synchronization. Each read or write of a volatile field acts like &amp;ldquo;half&amp;rdquo; a synchronization, for purposes of visibility.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; Note that it is important for both threads to access the same volatile variable in order to properly set up the happens-before relationship. It is not the case that everything visible to thread A when it writes volatile field &lt;code&gt;f&lt;/code&gt; becomes visible to thread B after it reads volatile field &lt;code&gt;g&lt;/code&gt;. The release and acquire have to &amp;ldquo;match&amp;rdquo; (i.e., be performed on the same volatile field) to have the right semantics.&lt;/p&gt;

&lt;h3 id=&#34;does-the-new-memory-model-fix-the-double-checked-locking-problem&#34;&gt;Does the new memory model fix the &amp;ldquo;double-checked locking&amp;rdquo; problem?&lt;/h3&gt;

&lt;p&gt;The (infamous) double-checked locking idiom (also called the multithreaded singleton pattern) is a trick designed to support lazy initialization while avoiding the overhead of synchronization. In very early JVMs, synchronization was slow, and developers were eager to remove it &amp;ndash; perhaps too eager. The double-checked locking idiom looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// double-checked-locking - don&#39;t do this!

private static Something instance = null;

public Something getInstance() {
  if (instance == null) {
    synchronized (this) {
      if (instance == null)
        instance = new Something();
    }
  }
  return instance;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This looks awfully clever &amp;ndash; the synchronization is avoided on the common code path. There&amp;rsquo;s only one problem with it &amp;ndash; &lt;strong&gt;it doesn&amp;rsquo;t work&lt;/strong&gt;. Why not? The most obvious reason is that the writes which initialize &lt;code&gt;instance&lt;/code&gt; and the write to the &lt;code&gt;instance&lt;/code&gt;field can be reordered by the compiler or the cache, which would have the effect of returning what appears to be a partially constructed &lt;code&gt;Something&lt;/code&gt;. The result would be that we read an uninitialized object. There are lots of other reasons why this is wrong, and why algorithmic corrections to it are wrong. There is no way to fix it using the old Java memory model. More in-depth information can be found at &lt;a href=&#34;http://www.javaworld.com/jw-02-2001/jw-0209-double.html&#34;&gt;Double-checked locking: Clever, but broken&lt;/a&gt; and &lt;a href=&#34;http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html&#34;&gt;The &amp;ldquo;Double Checked Locking is broken&amp;rdquo; declaration&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Many people assumed that the use of the &lt;code&gt;volatile&lt;/code&gt;keyword would eliminate the problems that arise when trying to use the double-checked-locking pattern. In JVMs prior to 1.5, &lt;code&gt;volatile&lt;/code&gt; would not ensure that it worked (your mileage may vary). Under the new memory model, making the &lt;code&gt;instance&lt;/code&gt; field volatile will &amp;ldquo;fix&amp;rdquo; the problems with double-checked locking, because then there will be a happens-before relationship between the initialization of the &lt;code&gt;Something&lt;/code&gt; by the constructing thread and the return of its value by the thread that reads it.&lt;/p&gt;

&lt;p&gt;~~However, for fans of double-checked locking (and we really hope there are none left), the news is still not good. The whole point of double-checked locking was to avoid the performance overhead of synchronization. Not only has brief synchronization gotten a LOT less expensive since the Java 1.0 days, but under the new memory model, the performance cost of using volatile goes up, almost to the level of the cost of synchronization. So there&amp;rsquo;s still no good reason to use double-checked-locking. ~~&lt;em&gt;Redacted &amp;ndash; volatiles are cheap on most platforms.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Instead, use the Initialization On Demand Holder idiom, which is thread-safe and a lot easier to understand:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static class LazySomethingHolder {
  public static Something something = new Something();
}

public static Something getInstance() {
  return LazySomethingHolder.something;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code is guaranteed to be correct because of the initialization guarantees for static fields; if a field is set in a static initializer, it is guaranteed to be made visible, correctly, to any thread that accesses that class.&lt;/p&gt;

&lt;h3 id=&#34;what-if-i-m-writing-a-vm&#34;&gt;What if I&amp;rsquo;m writing a VM?&lt;/h3&gt;

&lt;p&gt;You should look at&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://gee.cs.oswego.edu/dl/jmm/cookbook.html&#34;&gt;http://gee.cs.oswego.edu/dl/jmm/cookbook.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;h3 id=&#34;why-should-i-care&#34;&gt;Why should I care?&lt;/h3&gt;

&lt;p&gt;Why should you care? Concurrency bugs are very difficult to debug. They often don&amp;rsquo;t appear in testing, waiting instead until your program is run under heavy load, and are hard to reproduce and trap. You are much better off spending the extra effort ahead of time to ensure that your program is properly synchronized; while this is not easy, it&amp;rsquo;s a lot easier than trying to debug a badly synchronized application.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>